{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.example_input_array = torch.Tensor(64, 1, 28, 28)\n",
    "        self.encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "        self.decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.encoder(x)\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        return z\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        val_loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        # return val_loss\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        test_loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"test\", test_loss, prog_bar=True)\n",
    "        # return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = FashionMNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset, batch_size=64, num_workers=19,persistent_workers=True,pin_memory=True,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(dataset, len(dataset), num_workers=19, persistent_workers=True,pin_memory=True,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params | In sizes  | Out sizes\n",
      "---------------------------------------------------------------\n",
      "0 | encoder | Sequential | 50.4 K | [64, 784] | [64, 3]  \n",
      "1 | decoder | Sequential | 51.2 K | ?         | ?        \n",
      "---------------------------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759e3e66488b4fca96a9a60341157d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "# from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "# model\n",
    "autoencoder = LitAutoEncoder()\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"train_loss\", \n",
    "                                    min_delta=0.00, \n",
    "                                    patience=0, \n",
    "                                    verbose=True, \n",
    "                                    mode=\"min\")\n",
    "\n",
    "# train model\n",
    "trainer = L.Trainer(max_epochs=2, default_root_dir=os.path.join(os.getcwd(), 'lightning_ckpts'),\n",
    "                    callbacks=[], precision=\"16-mixed\", profiler=\"simple\")\n",
    "model=autoencoder\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf606631a8d4494b88b8c62ffb10c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           test            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.026734165847301483    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          test           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.026734165847301483   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test': 0.026734165847301483}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=autoencoder, dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6010 (pid 28848), started 0:02:05 ago. (Use '!kill 28848' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-12233403363d22a1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-12233403363d22a1\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6010;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./lightning_ckpts/lightning_logs/version_8/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b9bc7d7c934880ac8f601771b8b0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LitAutoEncoder.load_from_checkpoint(os.path.join(os.getcwd(), \n",
    "                                        'lightning_ckpts', 'lightning_logs',\n",
    "                                        'version_4', 'checkpoints', \n",
    "                                        'epoch=1-step=1876.ckpt'))\n",
    "# model.eval()\n",
    "# model(torch.rand(1, 28, 28, device=model.device))\n",
    "trainer = L.Trainer()\n",
    "predictions = trainer.predict(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 3])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def __init__(self, in_channels: int, out_channels: int, hidden_size: int, patch_size: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.positional_embeds = nn.Embedding(100000, hidden_size)\n",
    "        self.register_buffer(\"positional_ids\", torch.arange(100000).unsqueeze(0))\n",
    "        self.patch_conv = nn.Conv2d(in_channels, hidden_size, patch_size, stride=patch_size)\n",
    "        self.patch_deconv = nn.ConvTranspose2d(hidden_size, out_channels, patch_size, stride=patch_size)\n",
    "        self.model = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                hidden_size,\n",
    "                hidden_size // 64,\n",
    "                hidden_size * 8 // 3,\n",
    "                activation=F.silu,\n",
    "                batch_first=True,\n",
    "                norm_first=True,\n",
    "            ),\n",
    "            num_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        hidden = self.patch_conv(x)\n",
    "        n_seq = hidden.shape[2] * hidden.shape[3]\n",
    "        pos_embeds = self.positional_embeds(self.positional_ids[:, :n_seq]).expand(hidden.shape[0], -1, -1)\n",
    "        output = self.model(hidden.flatten(2).transpose(1, 2) + pos_embeds).transpose(-1, -2).view_as(hidden)\n",
    "        output = self.patch_deconv(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 8, 8])\n",
      "torch.Size([64, 64, 32])\n",
      "torch.Size([64, 32, 64])\n",
      "torch.Size([64, 64, 32])\n",
      "TRANSFORMER after torch.Size([64, 64, 32])\n",
      "torch.Size([64, 8, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 32, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "in_channels=8\n",
    "out_channels= 8\n",
    "hidden_size=32\n",
    "patch_size=(4, 4)\n",
    "num_layers=1\n",
    "a = torch.rand((64, 8, 32, 32))\n",
    "\n",
    "patch_conv = nn.Conv2d(in_channels, hidden_size, patch_size, stride=patch_size)\n",
    "a=patch_conv(a)\n",
    "# torch.Size([64, 32, 16, 16])\n",
    "print(a.shape)\n",
    "n_seq = a.shape[2] * a.shape[3]\n",
    "positional_embeds = nn.Embedding(100000, hidden_size)\n",
    "# register_buffer(\"positional_ids\", torch.arange(100000).unsqueeze(0))\n",
    "positional_ids = torch.arange(100000).unsqueeze(0).detach()\n",
    "pos_embeds = positional_embeds(positional_ids[:, :n_seq]).expand(a.shape[0], -1, -1)\n",
    "print(pos_embeds.shape)\n",
    "print(a.flatten(2).shape)\n",
    "print(a.flatten(2).transpose(1, 2).shape)\n",
    "a = a.flatten(2).transpose(1, 2) + pos_embeds\n",
    "model= nn.TransformerEncoder(\n",
    "    nn.TransformerEncoderLayer(\n",
    "        hidden_size,\n",
    "        2,\n",
    "        hidden_size * 8 // 3,\n",
    "        activation=F.silu,\n",
    "        batch_first=True,\n",
    "        norm_first=True,\n",
    "        ),\n",
    "    num_layers,\n",
    ")\n",
    "a = model(a)\n",
    "print(\"TRANSFORMER after\", a.shape)\n",
    "a= a.transpose(-1, -2)\n",
    "a =a.view_as(torch.rand((64, 32, 8, 8)))\n",
    "deconv = nn.ConvTranspose2d(hidden_size, out_channels, kernel_size=patch_size, stride=patch_size)\n",
    "a = deconv(a)\n",
    "print(a.shape)\n",
    "shao = nn.Conv2d(8, 8, kernel_size=(1, 32), stride=(1, 32))  # 从(32, 32)转换到(32, 1)，同时减少通道数至1\n",
    "# a=shao(a)\n",
    "# a.shape\n",
    "def _encoder_block(in_channel, out_channel, downsample):\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(in_channel, out_channel, kernel_size=(3, 3), stride=1, padding='same', bias=False))\n",
    "    layers.append(nn.BatchNorm2d(out_channel))\n",
    "    layers.append(nn.ReLU(inplace=False))\n",
    "    layers.append(nn.MaxPool2d(kernel_size=downsample, stride=downsample))\n",
    "    return nn.Sequential(*layers)\n",
    "ass = _encoder_block(8, 10, (1, 8))\n",
    "a = ass(a)\n",
    "ass = _encoder_block(10, 10, (1, 4))\n",
    "a = ass(a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((64, 32, 32, 1))\n",
    "x = x.view(x.size(0), x.size(1), x.size(2))\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义模型\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1)  # 从(16, 16)扩展到(32, 32)\n",
    "        self.conv = nn.Conv2d(32, 1, kernel_size=(32, 1), stride=1)  # 从(32, 32)转换到(32, 1)，同时减少通道数至1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.deconv(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# 输入张量\n",
    "input_tensor = torch.randn(64, 32, 16, 16)\n",
    "\n",
    "# 模型实例化和前向传播\n",
    "model = MyModel()\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "print(output_tensor.shape)  # 输出形状应为 torch.Size([64, 1, 32, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "patch_size = (2, 32)\n",
    "a = torch.rand((64, 32, 32, 32))\n",
    "nn = nn.Conv2d(32, 2, patch_size, stride=patch_size)\n",
    "a = nn(a)\n",
    "a.shape\n",
    "hidden = torch.rand((64, 64, 32, 1))\n",
    "hidden = hidden.flatten(2).transpose(1, 2)\n",
    "print(hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2, 32, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "patch_size = (1, 8)\n",
    "a = torch.rand((64, 32, 32, 64))\n",
    "n = nn.Conv2d(32, 2, patch_size, stride=patch_size)\n",
    "a = n(a)\n",
    "# a.shape\n",
    "\n",
    "n = nn.Conv2d(2, 2, patch_size, stride=patch_size)\n",
    "a = n(a)\n",
    "a.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
